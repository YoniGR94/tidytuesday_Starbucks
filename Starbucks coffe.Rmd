---
title: "Starbucks Coffee"
author: "Yoni Gâ€™etahun"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
subtitle: The Price is Right!
---

<p><strong>still in progress.</strong></p>


In this project, we are going to experience prediction in real life, with true unknown data. this data is the [Starbucks drink menu dataset](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-12-21),
from the [tidytuesday](https://github.com/rfordatascience/tidytuesday) project. This data was published in 21/12/2021.


```{r library, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(glmnet)
```

```{r install data, message=FALSE}
all_csvs <- list.files("2022/2021-12-21") |> 
  stringr::str_subset(".csv")

tuesdata <- tidytuesdayR::tt_load(2021, week = 52)
starbucks <- tuesdata$starbucks

```
This is going to be a blind test, so I won't have any clue about the calories in the test dataset. 

```{r 1st split}
set.seed(42)
base_split <- initial_split(starbucks, prop = 0.8)
cafe_train <- training(base_split)
cafe_test <- testing(base_split)

```

From now on, I will leave $cafe_test$ and create model based on $cafe_train$

```{r}
glimpse(cafe_train)
```

In the area of text handling, my strategy was to take the top 10 words. thoose words, like 'creme', 'caffe' can give us some clues in the future prediction.

```{r}

detector<- function(df)
{
  df<- df %>%  
    mutate(
      product_name = stringi::stri_trans_general(product_name, id = "Latin-ASCII") %>% str_to_lower())
  most_cmmon<- df %>% 
    select(product_name) %>% 
    str_replace_all('[:punct:]', " ") %>%
    str_squish() %>% 
    str_split('[ &]') %>% 
  table() %>% sort(decreasing = T)
  print(most_cmmon[1:10])
  most_cmmon[1:10] %>% names() %>% unlist()
}
```

Now we will see the most common words, after cleaning:

```{r}
common_10<- detector(cafe_train)

text_fun<- function(df, txtr)
  {
  df<- df %>%
    mutate(product_name = stringi::stri_trans_general(product_name, id = "Latin-ASCII") %>% str_to_lower())
  for (i in txtr)
  {
      df<- cbind(df, imap_lgl(df$product_name, ~str_detect(.x,i)) )
  }
  names(df)[(ncol(df)-9):ncol(df)]<- paste0('txt_',txtr)
  df %>% as.data.frame() #%>% select(-product_name)
}


detector(cafe_train)

cafe_train<- text_fun(cafe_train, common_10)
cafe_test<-  text_fun(cafe_test , common_10)

```

## Visualization and summarising data

How many drinks do we have in any size?

```{r}
cafe_train %>% count(size, sort = T)
```


```{r}
cafe_train %>% count(trans_fat_g)
```

The drinks with highest calories:

```{r}
cafe_train %>% arrange(-calories) %>% slice(1:5) %>% select(product_name, size, calories)
```
now by size:

```{r}
cafe_train %>% arrange(-calories) %>% 
  ggplot(aes(x= serv_size_m_l, y=calories,color= cut(saturated_fat_g,6) ))+
  geom_point(size= 0.8, alpha= 0.8)
```


The least The drinks with lowest calories:


```{r}
cafe_train %>% arrange(calories) %>% slice(1:10) %>% select(product_name, size, calories,txt_tea)

cafe_train %>% arrange(calories) %>% filter(calories== 0) %>% select(product_name, size, calories,txt_tea) %>% 
  count(size,txt_tea)

```

As we can see, all non calories drinks are Tea


```{r}
cafe_train %>% arrange(-calories) %>% 
  ggplot(aes(x= serv_size_m_l, y=calories,color= cut(saturated_fat_g,6) ))+
  geom_point(size= 0.8, alpha= 0.8)
```

## Prediction

```{r delete namer}
#cafe_train_ed<- cafe_train_ed %>% select(-product_name)
#cafe_test_ed<- cafe_test_ed %>% select(-product_name)
```

### RMSE Baseline

Let's do a basic split of the data we got

```{r, message=FALSE, warning=FALSE}
set.seed(42)
starbucks_split <- initial_split(cafe_train, prop = 0.8)
star_train_tr <- training(starbucks_split)
star_train_te <- testing(starbucks_split)
```

If we simply predict the training set mean...

```{r}
mean_calories <- mean(star_train_tr$calories)
mean_calories
rmse_vec(star_train_te$calories, rep(mean_calories, nrow(star_train_te)))

cbind(star_train_te$calories, rep(mean_calories, nrow(star_train_te))) %>%
  as.data.frame() %>%
  'colnames<-'(c('calories','mean_calories')) %>% 
  ggplot(aes(x=calories, y= calories-mean_calories))+
  geom_violin()
```

If we simply predict the mean of each category...

```{r}
mean_calories_cat <- star_train_tr %>%
  group_by(txt_tea,txt_frappuccino,txt_creme) %>%
  summarise(price_mean = mean(calories))

pred_price_cat <- star_train_te %>%
  inner_join(mean_calories_cat, by = c('txt_tea','txt_frappuccino','txt_creme')) %>%
  pull(price_mean)

rmse_vec(star_train_te$calories, pred_price_cat)

cbind(star_train_te, pred_price_cat) %>%
  as.data.frame() %>%
  
  #ggplot(aes(x=calories, y= calories-pred_price_cat, shape= txt_tea, color= txt_frappuccino, size= txt_creme))+
  #geom_point()
  ggplot(aes(x=calories, y= calories-pred_price_cat))+
  geom_violin()+facet_wrap(~txt_tea+txt_frappuccino+txt_creme)
```
We managed to slightly reduce the rmse, but not enough for a diet calculator.

Finally we will see where `NA` exist.

```{r}
naniar::vis_miss(star_train_tr)
```

Throwing in interaction between category and condition and is the product sent with free shipping, though almost all coefficients are "significant", doesn't really help RMSE:

```{r}
mod <- lm(calories ~ ., data = star_train_tr %>% select(-1))

pred_lm <- predict(mod, star_train_te %>% select(-1))

rmse_vec(star_train_te$calories, pred_lm)

cbind(star_train_te$calories, pred_lm) %>% 
  as.data.frame() %>%
  'colnames<-'(c('calories','pred_lm')) %>% 
  ggplot(aes(x=calories, y= pred_lm))+
    geom_smooth(se= F)+
    geom_point(alpha= 0.8, size= 0.8)

cbind(star_train_te$calories, pred_lm) %>% 
  as.data.frame() %>%
  'colnames<-'(c('calories','pred_lm')) %>% 
  ggplot(aes(sample = pred_lm))+
  stat_qq() + stat_qq_line()

```

## Heavy predictions


Now after our threshold was build, we will create a sensible model to predict the `calories` of the XXX drinks in `cafe_test`.

Only then we will see th result by cafe_test prediction RMS


```{r, eval=FALSE}
#cafe_test %>%
  #select(product_name, price_pred) %>%
  #write_csv("model01.csv")
```

#### use parnship

```{r}

dolche_de_leche<- function(df)
  {
  recipe(data=  df,calories~.) %>% 
  update_role(product_name, new_role = "id") %>% ## ADD THIS
  step_novel  (all_nominal(), -all_outcomes(), new_level = "the_rest")%>% 
  step_unknown(all_nominal(), new_level= "step_unknown" )%>%
  step_other  (all_nominal(), other = 'step_other', threshold = 0.004)%>% 
  step_normalize(all_numeric( ),-has_role("discard"),-has_role("discard"),-all_outcomes()) %>%
  step_nzv (all_numeric() ,-has_role("discard") ,-all_outcomes(),freq_cut = 99/1) %>% 
  step_dummy (all_nominal(), one_hot = F)
  }

bake_em_coffe<-  star_train_tr %>%dolche_de_leche()
test_baked<- star_train_te %>% dolche_de_leche()
  
set.seed(42)

cv_splits_star_train<- vfold_cv(star_train_tr, v = 10, strata = calories)
cv_splits <- vfold_cv(star_train_tr, v = 5) #for RF


cv_splits_star_train$bake_em_coffe <- map(
  cv_splits_star_train$splits, prepper, recipe = bake_em_coffe)

data_full<-      dolche_de_leche(cafe_train) %>% prep(cafe_train) %>% bake(new_data= cafe_train)
data_test<- dolche_de_leche(cafe_train) %>% prep(cafe_train) %>% bake(new_data= cafe_test)

```

### tune glmnet

```{r lr with punishment}
YG_model_1 <- linear_reg(penalty = 0.001, mixture = .5 ) %>%
  set_engine("glmnet")

fit_lr <- function(rec_obj) 
{
  fit(YG_model_1, calories ~ .,
      data = juice(rec_obj, all_predictors(), all_outcomes()))
}

cv_splits_star_train$YG_model_1<- map(cv_splits_star_train$bake_em_coffe,fit_lr)

pred_lr <- function(split_obj, rec_obj, model_obj) {
  mod_data <- bake(rec_obj,
                   new_data = assessment(split_obj),
                   all_predictors(), all_outcomes()) 
  out <- mod_data %>% select(calories)
  out$predicted <- predict(model_obj, mod_data, type = "numeric",
                           penalty = 0.0001)$.pred
  out
}

cv_splits_star_train$pred_unk <- 
  pmap(
    lst(
      split_obj = cv_splits_star_train$splits, 
      rec_obj = cv_splits_star_train$bake_em_coffe, 
      model_obj = cv_splits_star_train$YG_model_1), pred_lr)

cv_splits_star_train$pred_mode <- pmap(lst(
  split_obj = cv_splits_star_train$splits,
  rec_obj = cv_splits_star_train$bake_em_coffe,
  model_obj = cv_splits_star_train$YG_model_1), pred_lr)

train_recipe <- function(rec, rec_name, splits) {
  splits_prepped <- map(splits, prepper, recipe = rec)
  splits_fit <- map(splits_prepped, fit_lr)
  splits_pred <- pmap(
    lst(
      split_obj = splits, 
      rec_obj = splits_prepped, 
      model_obj = splits_fit ), pred_lr)
  res <- map_dfr(splits_pred, rmse, calories, predicted)$.pred
  names(res) <- rec_name
  res
}

#RME predict

RMSE<- matrix()
for (i in 1:10)
{
  RMSE[i]<- sum((cv_splits_star_train$pred_unk[[i]][,1]-cv_splits_star_train$pred_unk[[i]][,2])^2, na.rm = T)/
    dim(cv_splits_star_train$pred_unk[[i]])[1]
}

mean(RMSE)
RMSE

##try to bake at last
#____#
test_baked<- dolche_de_leche(cafe_train) %>% prep(cafe_train) %>% bake(cafe_test) 

mod_glmnet_final <- YG_model_1 %>%
  fit(calories ~ ., data = data_full)# %>%

mod_glmnet_final %>% 
  predict(new_data = data_test)

mod_glmnet_final %>% 
  predict(new_data = data_test) %>%   cbind(cafe_test$product_name) %>%
   rename(calories= 1)# %>% write_csv("true_model_glmnet_3fixe_pred_Yoni.csv")

```

#### random forest

```{r random forest}

YG_model_2 <- rand_forest(mode = "regression",
                           mtry = tune(), min_n = tune(),
                          trees = 100) %>%
  set_engine("ranger")


rf_grid <- grid_regular(mtry(range(4, 23)), min_n(range(10,30)), levels = c(4, 3))


tune_res <- tune_grid(YG_model_2,
                      bake_em_coffe,
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))

collect_metrics(tune_res)%>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_bw()

best_rmse <- tune_res %>% select_best(metric = "rmse")
best_rmse
mod_rf_final <- finalize_model(YG_model_2, best_rmse)

##try to bake at last
#____#
#???
test_baked<- dolche_de_leche(data_full) %>% prep(data_full) %>% bake(data_test) 

mod_rf_final$mod_fit_bake <- mod_rf_final %>%
  fit(calories ~ ., data = data_full)# %>%

mod_rf_final$mod_fit_bake %>% 
  predict(new_data = data_test) #%>%


#___#  
YG_model_2_tuned<- YG_model_2 <- rand_forest(mode = "regression",
                           mtry = best_rmse$mtry, min_n = best_rmse$min_n,
                          trees = 1000) %>%
  set_engine("ranger")


YG_model_2_tuned_fit<-
  YG_model_2_tuned %>%  fit_xy(x = bake_em_coffe$template[,-25],
                             y= bake_em_coffe$template$calories) 
#???
YG_model_2_tuned_pred<- YG_model_2_tuned_fit %>% predict(new_data = test_baked$template)


YG_model_2_tuned_pred<- cbind(YG_model_2_tuned_pred,test_baked$template$calories)
colnames(YG_model_2_tuned_pred)<- c('pred','true_calories')

mean((YG_model_2_tuned_pred$pred-YG_model_2_tuned_pred$true_calories)^2)
rmse_vec(YG_model_2_tuned_pred$pred,YG_model_2_tuned_pred$true_calories)


dim(YG_model_2_tuned_pred)
dim(test_baked$template)


```
#### preduction of random forest

```{r rf full to send}

rf_full<-      dolche_de_leche(shoes_train_YG) %>% prep(shoes_train_YG) %>% bake(new_data= shoes_train_YG)
rf_full_test<- dolche_de_leche(shoes_train_YG) %>% prep(shoes_train_YG) %>% bake(new_data= shoes_test_YG)

true_model_rf <- rand_forest(mode = "regression",
                           mtry = best_rmse$mtry, min_n = best_rmse$min_n,
                          trees = 300) %>% set_engine("ranger")

true_model_rf_set<- true_model_rf %>% 
  fit_xy(x = rf_full[,-(which(colnames(rf_full)== 'calories'))],
         y= rf_full$calories)

#true_model_rf_pred<- true_model_rf_set%>% bake(new_data = rf_full_test)

mod_rf_final$mod_fit_bake <- mod_rf_final %>%
  fit(calories ~ ., data = rf_full)# %>%

true_model_rf_to_csv<- mod_rf_final$mod_fit_bake %>% 
  predict(new_data = rf_full_test) #%>%


true_model_rf_to_csv %>%
  cbind(rf_full_test$id) %>% 
  rename(id= 'rf_full_test$id')# %>% 
  #select(id, price_pred) %>%
#  write_csv("true_model_rf_pred_Yoni.csv")

cbind(colnames(shoes_test_YG), colnames(shoes_train_YG) )
cbind(sort(colnames(rf_full_test$template)), sort(colnames(rf_full$template)))


cbind(colnames(rf_full_test), colnames(rf_full$template) )
rf_full_test

try_full_short<- mod_rf_final %>%
  fit(calories ~ ., data = shoes_train_tr)# %>%

try_full_short %>% 
predict(new_data = rf_full_test) %>%
  mutate(truth = shoes_train_te$calories)# %>%
  head(10)

```

### Hash models

Now I will add hash to the recipe and try again

```{r recipe with hash}

dolche_de_leche_hash<- function(df)
  {
  recipe(data=  df,calories~.) %>% 
  update_role(product_name, new_role = "id") %>% ## ADD THIS
  textrecipes::step_tokenize(product_name) %>%                                        #added hash
  textrecipes::step_texthash(product_name, signed = TRUE, num_terms = 12) %>%       #added hash
  update_role (starts_with("clr"),starts_with('texthash'),starts_with('selleler_notes'),new_role = "discard" )%>% 
  step_novel  (all_nominal(), -all_outcomes(), new_level = "the_rest")%>% 
  step_unknown(all_nominal(), new_level= "step_unknown" )%>%
  step_other  (all_nominal(), other = 'step_other', threshold = 0.004)%>% 
  step_normalize(all_numeric( ),-id,-has_role("discard"),-has_role("discard"),-all_outcomes()) %>%
  step_nzv (all_numeric(),-id ,-has_role("discard") ,-all_outcomes(),freq_cut = 99/1) %>% 
  step_dummy (all_nominal(),-has_role("discard"), one_hot = F) %>%
  update_role (starts_with("clr"),old_role =  "discard", new_role= "predictor" )
}

bake_em_hash_to_eat<- shoes_train_tr %>%dolche_de_leche_hash()
test_baked_hash<-     shoes_train_te %>% dolche_de_leche_hash()

cv_splits_star_train$bake_em_hash_to_eat <- map(
  cv_splits_star_train$splits, prepper, recipe = bake_em_hash_to_eat)

cafe_full_hash<-      dolche_de_leche_hash(shoes_train_YG) %>% prep(shoes_train_YG) %>% bake(new_data= shoes_train_YG)
cafe_full_test_hash<- dolche_de_leche_hash(shoes_train_YG) %>% prep(shoes_train_YG) %>% bake(new_data= shoes_test_YG)
```


```{r 2 hash glmnet}

YG_model_1_hasher <- linear_reg(penalty = 0.01, mixture = .5 ) %>%
  set_engine("glmnet")

fit_lr_hash <- function(rec_obj) 
{
  fit(YG_model_1_hasher, calories ~ .,
      data = juice(rec_obj, all_predictors(), all_outcomes()))
}

cv_splits_star_train$YG_model_1_hasher<- map(cv_splits_star_train$bake_em_hash_to_eat,fit_lr_hash)


cv_splits_star_train$pred_unk_hash <- 
  pmap(
    lst(
      split_obj = cv_splits_star_train$splits, 
      rec_obj = cv_splits_star_train$bake_em_hash_to_eat, 
      model_obj = cv_splits_star_train$YG_model_1_hasher), fit_lr_hash)

cv_splits_star_train$pred_mode_hash <- pmap(lst(
  split_obj = cv_splits_star_train$splits,
  rec_obj = cv_splits_star_train$bake_em_hash_to_eat,
  model_obj = cv_splits_star_train$YG_model_1_hasher), fit_lr_hash)


RMSE_08<- matrix()
for (i in 1:10)
{
  RMSE_08[i]<- sum((cv_splits_star_train$pred_unk[[i]][,1]-cv_splits_star_train$pred_unk[[i]][,2])^2, na.rm = T)/
    dim(cv_splits_star_train$pred_unk[[i]])[1]
}

mean(RMSE_08)
RMSE_08

##try to bake at last
#____#

mod_glmnet_final <- YG_model_1_hasher %>%
  fit(calories ~ ., data = cafe_full_hash)# %>%

mod_glmnet_final %>% 
  predict(new_data = cafe_full_test_hash)

mod_glmnet_final %>% 
  predict(new_data = cafe_full_test_hash) %>% 
  cbind(rf_full_test$id) %>%
   rename(id= 'rf_full_test$id') %>% 
  write_csv("YoniG_glhsh.csv")
#___#  


```


```{r hash RF before tune}
YG_model_RF_hash <- rand_forest(mode = "regression",
                           mtry = tune(), min_n = tune(),
                          trees = 100) %>%
  set_engine("ranger")


rf_grid <- grid_regular(mtry(range(15, 45)), min_n(range(10,30)), levels = c(4, 3))

tune_res <- tune_grid(YG_model_RF_hash,
                      bake_em_hash_to_eat,
                      resamples = cv_splits,
                      grid = rf_grid,
                      metrics = metric_set(rmse))

collect_metrics(tune_res)%>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(x = mtry, y = mean, color = min_n)) + 
  geom_point() + 
  geom_line() + 
  labs(y = "Mean RMSE") +
  theme_bw()

best_rmse_RF_hash <- tune_res %>% select_best(metric = "rmse")
best_rmse_RF_hash
mod_rf_final_RF_hash <- finalize_model(YG_model_RF_hash, best_rmse_RF_hash)

##try to bake at last
#____#
RF_hash_baked<- dolche_de_leche_hash(shoes_train_YG) %>% prep(shoes_train_YG) %>% bake(shoes_test_YG) 

#___#  

```


### tuned random forest

```{r hash RF tuned}



#true_model_rf_hash <- rand_forest(mode = "regression",
#                           mtry = best_rmse_RF_hash$mtry, min_n = best_rmse_RF_hash$min_n,
#                          trees = 1000) %>% set_engine("ranger")

true_model_rf_hash <- rand_forest(mode = "regression",
                           mtry = 30, min_n = 15,
                          trees = 800) %>% set_engine("ranger")

true_model_rf_hash_set<- true_model_rf_hash %>% 
  fit_xy(x = cafe_full_hash[,-(which(colnames(cafe_full_hash)== 'calories'))],
         y= cafe_full_hash$calories)

mod_rfhash_final <- finalize_model(true_model_rf_hash, best_rmse_RF_hash)
#library(text2vec)
detach("package:text2vec", unload = T)                       # Detach stringr packagetext
true_model_rf_hash$mod_RFhash_bake <- true_model_rf_hash %>%
  fit(calories ~ ., data = cafe_full_hash) #%>%

true_model_rf_hash_to_csv<-  
  predict(true_model_rf_hash_set ,new_data = cafe_full_test_hash) #%>%


true_model_rf_hash_to_csv %>%
  cbind(rf_full_test$id) %>% 
  rename(id= 'rf_full_test$id') %>% 
  write_csv("true_model_RFhsh_Yoni.csv")

  
```


### keras